library(class)
trainX = train_set[,-16]
testX = test_set[-16]
#########################Test-Train Split#############################
#setting a seed
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
library(class)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
k=3
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
knn.pred = knn(trainX_s,testX_s,trainY,k=k)
#Confusion Matrix
table(knn.pred,testY)
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
barplot(table(df$airport))
barplot(table(df$bus_ter))
# Treating NA values
which(is.na(df$n_hos_beds))
# EDD (extended data dictionary)
summary(df)
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
glm.fit = glm(Sold~., data = df,family = binomial)
summary(glm.fit)
glm.probs = predict(glm.fit,type = "response") # predicting problity of the model on the data
glm.probs[1:10]
glm.pred = rep("No",506) # creating an array which consist of 506 "No values"
# Now wherever the probablity is > 0.5 will change the "No" to "Yes"
glm.pred[glm.probs>0.5] = "Yes"
#Confusion Matrix
table(glm.pred,df$Sold)
# LDA classifier
library(MASS)
lda.fit = lda(Sold~.,data = df)
lda.fit
lda.pred = predict(lda.fit,df)
# To look at the posterior probablity of the variables
lda.pred$posterior
# By default the probablity condition is of 0.5
lda.class = lda.pred$class
# confusion matrix
table(lda.class,df$Sold)
# To change the boundary condition of the probablity on which the class are assigned
sum(lda.pred$posterior[,1]>0.8) # To give the count where the lda.pred$posterior variables 2nd column is > 0.8
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
glm.fit = glm(Sold~., data = df,family = binomial)
summary(glm.fit)
glm.probs = predict(glm.fit,type = "response") # predicting problity of the model on the data
glm.probs[1:10]
glm.pred = rep("No",506) # creating an array which consist of 506 "No values"
# Now wherever the probablity is > 0.5 will change the "No" to "Yes"
glm.pred[glm.probs>0.5] = "Yes"
#Confusion Matrix
table(glm.pred,df$Sold)
# LDA classifier
library(MASS)
lda.fit = lda(Sold~.,data = df)
lda.fit
lda.pred = predict(lda.fit,df)
# To look at the posterior probablity of the variables
lda.pred$posterior
# By default the probablity condition is of 0.5
lda.class = lda.pred$class
# confusion matrix
table(lda.class,df$Sold)
# To change the boundary condition of the probablity on which the class are assigned
sum(lda.pred$posterior[,1]>0.8) # To give the count where the lda.pred$posterior variables 2nd column is > 0.8
library(caTools)
#########################Test-Train Split#############################
#setting a seed
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
library(class)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
k=3
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
knn.pred = knn(trainX_s,testX_s,trainY,k=k)
#Confusion Matrix
table(knn.pred,testY)
# Comparasion of all 3 Classification Model
if (!require(rpart)) install.packages('rpart')
library(rpart)
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
tree1 <- rpart(Sold ~ ., data=trainX_s, method = 'anova')
tree1 <- rpart(Sold ~ ., data=train_set, method = 'anova')
fancyRpartPlot(tree1, caption = "Regression Tree")
library(caret)
install.packages("caret")
library(caret)
(varimp.tree1 <- varImp(tree1))
dtest$tree1.pred <- predict(tree1, newdata = test_set)
test_set$tree1.pred <- predict(tree1, newdata = test_set)
head(test_set[c("mpg","tree1.pred")], n=nrow(test_set))
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set)
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
test_set$tree1.pred <- predict(tree1, newdata = test_set)
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(tree1.pred,high.Sold)))
(cm2 <- with(test_set,table(tree1.pred,Sold)))
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
library(caret)
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set)
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(tree1.pred,Sold)))
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
#install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
library(rpart)
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
library(caTools)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
library(caret)
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set)
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(tree1.pred,Sold)))
tree1
View(test_set)
(cm2 <- with(test_set,table(test_set$tree1.pred,test_set$Sold)))
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
library(caret)
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set,type = 'class')
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(test_set$tree1.pred,test_set$Sold)))
View(test_set)
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
#install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
glm.fit = glm(Sold~., data = df,family = binomial)
summary(glm.fit)
glm.probs = predict(glm.fit,type = "response") # predicting problity of the model on the data
glm.probs[1:10]
glm.pred = rep("No",506) # creating an array which consist of 506 "No values"
# Now wherever the probablity is > 0.5 will change the "No" to "Yes"
glm.pred[glm.probs>0.5] = "Yes"
#Confusion Matrix
table(glm.pred,df$Sold)
library(caTools)
#########################Test-Train Split#############################
#setting a seed
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
library(class)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
k=3
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
knn.pred = knn(trainX_s,testX_s,trainY,k=k)
#Confusion Matrix
table(knn.pred,testY)
# Comparasion of all 3 Classification Model
# Comparasion of all 3 Classification Model
# Logistic Regression 65% accuracy
# Comparasion of all 3 Classification Model
# Logistic Regression 65% accuracy
# KNN 55% accuracy on test data
if (!require(rpart)) install.packages('rpart')
library(rpart)
library(caTools)
set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
library(caret)
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set,type = 'class')
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(test_set$tree1.pred,test_set$Sold)))
View(test_set)
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$bus_ter))
glm.probs[1:10]
#Confusion Matrix
table(glm.pred,df$Sold)
#Confusion Matrix
table(knn.pred,testY)
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
#install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
barplot(table(df$bus_ter))
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
str(df) # will give structure of df
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
#install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
barplot(table(df$bus_ter))
barplot(table(df$airport))
barplot(table(df$bus_ter))
df <- read.csv("HousePrice.csv",header = TRUE)
View(df)
# EDD (extended data dictionary)
summary(df)
# Graphs
boxplot(df$n_hot_rooms) # here can see outliers
pairs(~df$Sold+df$rainfall) # can see outlier
barplot(table(df$airport))
barplot(table(df$bus_ter))
uv<- 3*quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms>uv] <- uv
lv <- 0.3*quantile(df$rainfall,0.01)
df$rainfall[df$rainfall<lv] <- lv
# Treating NA values
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds,na.rm = TRUE)
mean(df$n_hos_beds)
df$avg_dist = (df$dist1+df$dist2+df$dist3+df$dist4)/4  # taking out the average and creating a column with it
tempdf <- df[,-6:-9]# df without column 6 to 9
df = tempdf
rm(tempdf) # remover temp variable
df <- df[,-13] # removing the not usefull columns of bus_ter
#install.packages("dummies")
library(dummies)
df<- dummy.data.frame(df)
df<- df[,-8]
df<-df[,-13]
glm.fit = glm(Sold~., data = df,family = binomial)
summary(glm.fit)
glm.probs = predict(glm.fit,type = "response") # predicting problity of the model on the data
glm.probs[1:10]
glm.pred = rep("No",506) # creating an array which consist of 506 "No values"
# Now wherever the probablity is > 0.5 will change the "No" to "Yes"
glm.pred[glm.probs>0.5] = "Yes"
#Confusion Matrix
table(glm.pred,df$Sold)
tree1 <- rpart(Sold ~ ., data=train_set, method = 'class')
library(caret)
(varimp.tree1 <- varImp(tree1))
test_set$tree1.pred <- predict(tree1, newdata = test_set,type = 'class')
head(test_set[c("Sold","tree1.pred")], n=nrow(test_set))
(cm2 <- with(test_set,table(test_set$tree1.pred,test_set$Sold)))
library(caTools)
#########################Test-Train Split#############################
#setting a seed
#set.seed(0)
split = sample.split(df,SplitRatio = 0.8)
train_set = subset(df,split==TRUE)
test_set = subset(df,split==FALSE)
library(class)
trainX = train_set[,-16]
testX = test_set[-16]
trainY = train_set$Sold
testY = test_set$Sold
k=3
# standardise=ing the variables
trainX_s = scale(trainX)
testX_s = scale(testX)
set.seed(0)
knn.pred = knn(trainX_s,testX_s,trainY,k=k)
#Confusion Matrix
table(knn.pred,testY)
